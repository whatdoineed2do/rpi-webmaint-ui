# Spam blocker v2.9 by Charlton Trezevant

# Disable Direct IP access to the server
# Enable this if you have a FQDN. This will stop most scanners.
# You may also want to consider redirecting to your FQDN rather than serving a 403.
# This should be enabled but is set to ctis.me's IP by default so things aren't automagically broken if you forget ;)
$HTTP["host"] == "104.131.40.203" { url.access-deny = ( "" ) accesslog.filename = "/var/log/lighttpd/spam.log"}


### Block spam crawlers!
# Empty UA, or -.
#This blocks a lot of spam crawlers and bad robots. 
$HTTP["useragent"] == "" { url.access-deny = ( "" ) accesslog.filename = "/var/log/lighttpd/spam.log"}
$HTTP["useragent"] == "-" { url.access-deny = ( "" ) accesslog.filename = "/var/log/lighttpd/spam.log"}

# Block bad requsts with empty "Host" headers.
$HTTP["host"] == "" {url.access-deny = ( "" ) accesslog.filename = "/var/log/lighttpd/spam.log"}
$HTTP["host"] == "-" { url.access-deny = ( "" ) accesslog.filename = "/var/log/lighttpd/spam.log"}

# Deny all URLs to likely malicious UAs (minimizing access to potentially vulnerable apps).
$HTTP["useragent"] =~ "perl|\{|\}|\/var\/|\/tmp\/|\/etc\/|China.Z|ZmEu|Zollard|gawa\.sa\.pilipinas|Jorgee" { url.access-deny = ( "" ) accesslog.filename = "/var/log/lighttpd/spam.log"}

# Spammy SEO companies
# Some UAs sourced from http://www.iplists.com/non_engines.txt
$HTTP["useragent"] =~ "thunderstone|PageAnalyzer|Netcraft|5118\.com|MJ12bot|majestic12\.co\.uk|dotbot|LSSRocketCrawler|LightspeedSystems|StatsInfo|ips-agent|TurnitinBot|SlySearch|ScoutAbout|RPT-HTTPClient|WebZIP|Webclipping\.com|webrank|websquash\.com|lwp-trivial|AESOP_com_SpiderMan|Zeus|Cyveillance|Lite\ Bot|flunky|Microsoft\ URL\ Control|NAMEPROTECT|NPBot|aipbot|WebFilter|Indy\ Library|SurveyBot|Morfeus\ Fucking\ Scanner|probethenet\.com|digitalmole|AhrefsBot|linkfluence\.net|Kraken|datasift\.com|DomainSigmaCrawler|AdnormCrawler|adnorm\.com|OptimizationCrawler|domainoptima\.com|woriobot|worio\.com|zitebot|coccoc|coccoc.com|Mnogosearch|Synapse" { url.access-deny = ( "" ) accesslog.filename = "/var/log/lighttpd/spam.log" }
$HTTP["referer"] =~ "semalt|-seo|darodar\.com|makemoneyonline\.com|rankings-analytics\.com|D-SHANGZJ\.COM|for-website\.com|for-your-website\.com|social-buttons\.com|free-share-buttons\.com|buy-cheap-online\.info|24x7-allrequestsallowed\.com|domainsigma\.com|for-your|video--production\.com|justprofit\.xyz|hundejo\.com|pizza-tycoon\.com|pizza-imperia\.com|buypillsorderonline\.com" { url.access-deny =  ( "" ) accesslog.filename = "/var/log/lighttpd/spam.log" }

# Net Scanners, including https://github.com/robertdavidgraham/masscan.
$HTTP["useragent"] =~ "netscan.lekus.su|pdrlabs.net|massscan|ip138.com|nerd.host" { url.access-deny = ( "" ) accesslog.filename = "/var/log/lighttpd/spam.log" }

# Some of these IPs are also Baidu, but none of them respect robots.txt and 
# simply change the user-agents of so as to circumvent blocking. Of course, we can fix that :)
# Blocked IPs:
# 123.125.71.x  220.181.108.x 185.10.107.x - Baidu Spiders
# 202.46.*.* - Blocks bots @ ptr.cnsat.com.cn 
# 180.76.15.*, 185.10.107.* - More Baidu
$HTTP["remoteip"] =~ "123\.125\.71\.*|220\.181\.108\.*|202\.46\.*.*|180.76.15.*|185.10.107.*" { url.access-deny = ( "" ) accesslog.filename = "/var/log/lighttpd/spam.log" }

# Baidu, Sogou, YoDao, Haosou, and Yisou, all of whom disregard robots.txt
$HTTP["useragent"] =~ "Baidu|Baiduspider|Sogou|YoudaoBot|YodaoBot|HaoSouSpider|360Spider|YisouSpider" { url.access-deny = ( "" ) accesslog.filename = "/var/log/lighttpd/spam.log" }

# Always allow access to robots.txt, to everyone.
$HTTP["url"] =~ "^/robots.txt"{ url.access-deny = ("disable") }
